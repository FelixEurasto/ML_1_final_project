{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6b34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn import tree, svm, preprocessing\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE, RFECV, SelectFromModel, SelectKBest, chi2, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b94240a",
   "metadata": {},
   "source": [
    "## Data import and proprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adab3530",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import trainning and test sets\n",
    "training_data = pd.DataFrame(pd.read_csv('../Downloads/npf_train.csv')).drop(['id', 'date', 'partlybad'], axis=1)\n",
    "test_data = pd.DataFrame(pd.read_csv('../Downloads/npf_test_hidden.csv')).drop(['id', 'date', 'partlybad', 'class4'], axis=1)\n",
    "\n",
    "\n",
    "# Make new categorical 'type' variable whose values are integers instead of strings\n",
    "\n",
    "training_types = []\n",
    "for i, x in enumerate(training_data['class4']):\n",
    "\n",
    "    if x == 'nonevent':\n",
    "        training_types.append(0)\n",
    "    if x == 'Ia':\n",
    "        training_types.append(1)\n",
    "    if x == 'Ib':\n",
    "        training_types.append(2)\n",
    "    if x == 'II':\n",
    "        training_types.append(3)\n",
    "        \n",
    "\n",
    "training_data['type'] = training_types\n",
    "\n",
    "training_data['class2'] = [0 if i == 'nonevent' else 1 for i in training_data['class4']] # Make new categorical 'class2' variable whose\n",
    "                                                                                        #value is 1 for 'event' and 0 for 'nonevent'\n",
    "\n",
    "training_data = training_data.drop('class4', axis=1) # Drop redundent 'class4' variable\n",
    "\n",
    "features = list(training_data.columns) # Define list of features from which to choose best ones for the models\n",
    "\n",
    "\n",
    "features = features[:-2] # Drop variables that we aim to predict from this list\n",
    "\n",
    "\n",
    "# Scale the values of the features to have zero mean and unit variance to ensure sklearn methods work properly\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(training_data[features])\n",
    "scaler1 = preprocessing.StandardScaler().fit(test_data[features])\n",
    "training_data[features] = scaler.transform(training_data[features])\n",
    "test_data[features] = scaler.transform(test_data[features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a818d",
   "metadata": {},
   "source": [
    "## Choosing which features to train the binary classifier on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a895a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use iterative feature pruning to find optimal set of features for logistic regression model\n",
    "# Logistic regression chosen here just to to find a set of good features\n",
    "\n",
    "model = RFECV(LogisticRegression(random_state=0, max_iter=1000), step = 1, cv = 5, min_features_to_select=25)\n",
    "\n",
    "model.fit(training_data[features], training_data['class2']) # Fit the model using training data\n",
    "\n",
    "means = [np.mean(i) for i in model.grid_scores_] # Mean CV accururacies of models fit on different sets of features\n",
    "\n",
    "    \n",
    "\n",
    "probs = model.predict_proba(test_data[features])\n",
    "preds = model.predict(test_data[features])\n",
    "\n",
    "# Plot the mean accuracies of models fit on different sets of features\n",
    "\n",
    "plt.plot([_ + 25 for _ in range(len(means))], means, alpha=0.5)\n",
    "\n",
    "plt.scatter([_ + 25 for _ in range(len(means))], means, s=10)\n",
    "plt.scatter(41, means[16], c='r', s=14)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of features')\n",
    "\n",
    "binary_used = [e for i, e in enumerate(model.feature_names_in_) if model.support_[i] == True] # Save used features in list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02ab348",
   "metadata": {},
   "source": [
    "## Choosing which features to train the multiclass classifier on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8df81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same procedure as with the binary classifier\n",
    "\n",
    "model1 = RFECV(LogisticRegression(random_state=0, max_iter=1000), step = 1, cv = 5, min_features_to_select=10)\n",
    "model1.fit(training_data[features], training_data['type'])\n",
    "\n",
    "\n",
    "means1 = [np.mean(i) for i in model1.grid_scores_]\n",
    "\n",
    "multiclass_used = [e for i, e in enumerate(model1.feature_names_in_) if model1.support_[i] == True]\n",
    "\n",
    "\n",
    "# Plot the mean accuracies of models fit on different sets of features\n",
    "\n",
    "plt.scatter([_ + 10 for _ in range(len(means1))], means1, s=10)\n",
    "plt.plot([_ + 10 for _ in range(len(means1))], means1, alpha=0.5)\n",
    "plt.scatter(means1.index(max(means1)) + 10, means1[means1.index(max(means1))], c='r', s=14)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of features')\n",
    "plt.savefig('logistic_accuracies_multi.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb7901",
   "metadata": {},
   "source": [
    "## Deciding which kind of model to use for the binary classification\n",
    "## Considered models: logistic regression, naive Bayes, SVM, k-NN and decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06403603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folds = KFold(n_splits=10)\n",
    "\n",
    "a = []\n",
    "b = []\n",
    "c = []\n",
    "d = []\n",
    "e = []\n",
    "\n",
    "\n",
    "for tr, te in folds.split(training_data):\n",
    "    \n",
    "    model = LogisticRegression(random_state=0, max_iter=1000)\n",
    "    \n",
    "    model.fit(training_data[features].iloc[tr], training_data['class2'].iloc[tr])\n",
    "    \n",
    "    a.append(model.score(training_data[features].iloc[te], training_data['class2'].iloc[te]))\n",
    "    \n",
    "\n",
    "for tr, te in folds.split(training_data):\n",
    "    \n",
    "    model = GaussianNB()\n",
    "    \n",
    "    model.fit(training_data[features].iloc[tr], training_data['class2'].iloc[tr])\n",
    "    \n",
    "    b.append(model.score(training_data[features].iloc[te], training_data['class2'].iloc[te]))\n",
    "\n",
    "\n",
    "for tr, te in folds.split(training_data):\n",
    "    \n",
    "    model = svm.SVC(kernel='rbf')\n",
    "    \n",
    "    model.fit(training_data[features].iloc[tr], training_data['class2'].iloc[tr])\n",
    "    \n",
    "    c.append(model.score(training_data[features].iloc[te], training_data['class2'].iloc[te]))\n",
    "    \n",
    "\n",
    "\n",
    "for tr, te in folds.split(training_data):\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors=20)\n",
    "    \n",
    "    model.fit(training_data[features].iloc[tr], training_data['class2'].iloc[tr])\n",
    "    \n",
    "    d.append(model.score(training_data[features].iloc[te], training_data['class2'].iloc[te]))\n",
    "\n",
    "\n",
    "\n",
    "for tr, te in folds.split(training_data):\n",
    "    \n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    \n",
    "    model.fit(training_data[features].iloc[tr], training_data['class2'].iloc[tr])\n",
    "    \n",
    "    e.append(model.score(training_data[features].iloc[te], training_data['class2'].iloc[te]))\n",
    "    \n",
    "    \n",
    "\n",
    "scores = ['score', np.round(np.mean(b), 3), np.round(np.mean(a), 3), np.round(np.mean(c), 3),\n",
    "          np.round(np.mean(d), 3), np.round(np.mean(e), 3)]\n",
    "models = ['model', 'naive Bayes', 'logistic regression', 'SVM', 'k-NN', 'decision tree']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "table = [models, scores]\n",
    "\n",
    "table = tabulate(table, tablefmt='fancy')\n",
    "\n",
    "print(table)\n",
    "\n",
    "\n",
    "# Let's choose a logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63df888",
   "metadata": {},
   "source": [
    "## Deciding which kind of model to use for the multiclass classification\n",
    "## Considered models: logistic regression, naive Bayes, SVM, k-NN and decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e18f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folds = KFold(n_splits=10)\n",
    "\n",
    "a = []\n",
    "b = []\n",
    "c = []\n",
    "d = []\n",
    "e = []\n",
    "\n",
    "\n",
    "for tr, te in folds.split(training_data):\n",
    "    \n",
    "    model = LogisticRegression(random_state=0, max_iter=1000)\n",
    "    \n",
    "    model.fit(training_data[features].iloc[tr], training_data['type'].iloc[tr])\n",
    "    \n",
    "    a.append(model.score(training_data[features].iloc[te], training_data['type'].iloc[te]))\n",
    "    \n",
    "\n",
    "for tr, te in folds.split(training_data):\n",
    "    \n",
    "    model = GaussianNB()\n",
    "    \n",
    "    model.fit(training_data[features].iloc[tr], training_data['type'].iloc[tr])\n",
    "    \n",
    "    b.append(model.score(training_data[features].iloc[te], training_data['type'].iloc[te]))\n",
    "\n",
    "\n",
    "for tr, te in folds.split(training_data):\n",
    "    \n",
    "    model = svm.SVC(kernel='rbf')\n",
    "    \n",
    "    model.fit(training_data[features].iloc[tr], training_data['type'].iloc[tr])\n",
    "    \n",
    "    c.append(model.score(training_data[features].iloc[te], training_data['type'].iloc[te]))\n",
    "    \n",
    "\n",
    "\n",
    "for tr, te in folds.split(training_data):\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors=20)\n",
    "    \n",
    "    model.fit(training_data[features].iloc[tr], training_data['type'].iloc[tr])\n",
    "    \n",
    "    d.append(model.score(training_data[features].iloc[te], training_data['type'].iloc[te]))\n",
    "\n",
    "\n",
    "\n",
    "for tr, te in folds.split(training_data):\n",
    "    \n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    \n",
    "    model.fit(training_data[features].iloc[tr], training_data['type'].iloc[tr])\n",
    "    \n",
    "    e.append(model.score(training_data[features].iloc[te], training_data['type'].iloc[te]))\n",
    "    \n",
    "    \n",
    "\n",
    "scores = ['score', np.round(np.mean(b), 3), np.round(np.mean(a), 3), np.round(np.mean(c), 3),\n",
    "          np.round(np.mean(d), 3), np.round(np.mean(e), 3)]\n",
    "models = ['model', 'naive Bayes', 'logistic regression', 'SVM', 'k-NN', 'decision tree']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "table = [models, scores]\n",
    "\n",
    "table = tabulate(table, tablefmt='fancy')\n",
    "\n",
    "print(table)\n",
    "\n",
    "\n",
    "# Let's choose a logistic regression model, since REFCV cannot be used for SVM or k-NN\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d8dd4",
   "metadata": {},
   "source": [
    "## Checking accuracy of developed models and calculating perplexity of binary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace57f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "true_data = pd.DataFrame(pd.read_csv('../Downloads/npf_test.csv'))\n",
    "true_data['class2'] = [0 if t == 'nonevent' else 1 for t in true_data['class4']]\n",
    "true_types = []\n",
    "true_class2 = []\n",
    "\n",
    "\n",
    "for i, x in enumerate(true_data['class4']):\n",
    "\n",
    "    if x == 'nonevent':\n",
    "        true_types.append(0)\n",
    "    if x == 'Ia':\n",
    "        true_types.append(1)\n",
    "\n",
    "    if x == 'Ib':\n",
    "        true_types.append(2)\n",
    "\n",
    "    if x == 'II':\n",
    "        true_types.append(3)\n",
    "\n",
    "        \n",
    "\n",
    "true_data['type'] = true_types\n",
    "\n",
    "\n",
    "\n",
    "model = LogisticRegression(random_state=0, max_iter=1000).fit(training_data[binary_used], training_data['class2'])\n",
    "\n",
    "binary_preds = model.predict(test_data[binary_used])\n",
    "\n",
    "model1 = LogisticRegression(random_state=0, max_iter=1000).fit(training_data[multiclass_used], training_data['type'])\n",
    "\n",
    "multi_preds = model1.predict(test_data[multiclass_used])\n",
    "\n",
    "multi_preds = [x if x == 0 else multi_preds[i] for i, x in enumerate(binary_preds)]\n",
    "\n",
    "multi_accuracy = len([x for i, x in enumerate(multi_preds) if x == true_data['type'][i]]) / len(multi_preds)\n",
    "\n",
    "\n",
    "\n",
    "print('Binary accuracy: ' + str(model.score(test_data[binary_used], true_data['class2'])))\n",
    "print('Multiclass accuracy: ' + str(multi_accuracy))\n",
    "\n",
    "\n",
    "#Calculating peroplexity of the binary classifier\n",
    "u = [model.predict_proba(test_data[binary_used])[i][0] if model.predict(test_data[binary_used])[i] == 0 else \n",
    "    model.predict_proba(test_data[binary_used])[i][1] for i in range(test_data.shape[0])]\n",
    "\n",
    "perp = np.exp(-np.mean([np.log(i) for i in u]))\n",
    "\n",
    "print('Perplexity: ' + str(perp))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
